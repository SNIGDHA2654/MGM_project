{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45c4e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AGARW\\anaconda3\\envs\\mypy\\lib\\site-packages\\torch\\utils\\cpp_extension.py:316: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "  warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AGARW\\OneDrive\\Documents\\GitHub\\MGM_project\\semanticGAN\\train_seg_gan.py\", line 38, in <module>\n",
      "    from models.stylegan2_seg import GeneratorSeg, Discriminator, MultiscaleDiscriminator, GANLoss\n",
      "  File \"C:\\Users\\AGARW\\OneDrive\\Documents\\GitHub\\MGM_project\\semanticGAN\\..\\models\\stylegan2_seg.py\", line 11, in <module>\n",
      "    from models.utils import *\n",
      "  File \"C:\\Users\\AGARW\\OneDrive\\Documents\\GitHub\\MGM_project\\semanticGAN\\..\\models\\utils.py\", line 12, in <module>\n",
      "    from models.op import FusedLeakyReLU, fused_leaky_relu, upfirdn2d\n",
      "  File \"C:\\Users\\AGARW\\OneDrive\\Documents\\GitHub\\MGM_project\\semanticGAN\\..\\models\\op\\__init__.py\", line 1, in <module>\n",
      "    from .fused_act import FusedLeakyReLU, fused_leaky_relu\n",
      "  File \"C:\\Users\\AGARW\\OneDrive\\Documents\\GitHub\\MGM_project\\semanticGAN\\..\\models\\op\\fused_act.py\", line 17, in <module>\n",
      "    fused = load(\n",
      "  File \"C:\\Users\\AGARW\\anaconda3\\envs\\mypy\\lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1124, in load\n",
      "    return _jit_compile(\n",
      "  File \"C:\\Users\\AGARW\\anaconda3\\envs\\mypy\\lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1337, in _jit_compile\n",
      "    _write_ninja_file_and_build_library(\n",
      "  File \"C:\\Users\\AGARW\\anaconda3\\envs\\mypy\\lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1436, in _write_ninja_file_and_build_library\n",
      "    _write_ninja_file_to_build_library(\n",
      "  File \"C:\\Users\\AGARW\\anaconda3\\envs\\mypy\\lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1834, in _write_ninja_file_to_build_library\n",
      "    cuda_flags = common_cflags + COMMON_NVCC_FLAGS + _get_cuda_arch_flags()\n",
      "  File \"C:\\Users\\AGARW\\anaconda3\\envs\\mypy\\lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1606, in _get_cuda_arch_flags\n",
      "    arch_list[-1] += '+PTX'\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "!python train_seg_gan.py \\\n",
    "--img_dataset 'C:/Users/AGARW/Downloads/CelebAMask-HQ/CelebAMask-HQ/' \\\n",
    "--seg_dataset 'C:/Users/AGARW/Downloads/CelebAMask-HQ/CelebAMask-HQ/' \\\n",
    "--inception 'C:/Users/AGARW/OneDrive/Documents/GitHub/MGM_project/inception_output.pkl' \\\n",
    "--seg_name celeba-mask \\\n",
    "--checkpoint_dir 'C:/Users/AGARW/OneDrive/Documents/GitHub/MGM_project/checkpoint/' \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (C) 2021 NVIDIA Corporation.  All rights reserved.\n",
    "Licensed under The MIT License (MIT)\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, autograd, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import transforms, utils\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from models.stylegan2_seg import GeneratorSeg, Discriminator, MultiscaleDiscriminator, GANLoss\n",
    "from dataloader.dataset import CelebAMaskDataset\n",
    "\n",
    "from utils.distributed import (\n",
    "    get_rank,\n",
    "    synchronize,\n",
    "    reduce_loss_dict,\n",
    "    reduce_sum,\n",
    "    get_world_size,\n",
    ")\n",
    "\n",
    "import functools\n",
    "from utils.inception_utils import sample_gema, prepare_inception_metrics\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "def data_sampler(dataset, shuffle, distributed):\n",
    "    if distributed:\n",
    "        return data.distributed.DistributedSampler(dataset, shuffle=shuffle)\n",
    "\n",
    "    if shuffle:\n",
    "        return data.RandomSampler(dataset)\n",
    "\n",
    "    else:\n",
    "        return data.SequentialSampler(dataset)\n",
    "\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n",
    "\n",
    "\n",
    "def sample_data(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "\n",
    "\n",
    "def d_logistic_loss(real_pred, fake_pred):\n",
    "    real_loss = F.softplus(-real_pred)\n",
    "    fake_loss = F.softplus(fake_pred)\n",
    "\n",
    "    return real_loss.mean() + fake_loss.mean()\n",
    "\n",
    "\n",
    "def d_r1_loss(real_pred, real_img):\n",
    "    grad_real, = autograd.grad(\n",
    "        outputs=real_pred.sum(), inputs=real_img, create_graph=True\n",
    "    )\n",
    "    grad_penalty = grad_real.pow(2).reshape(grad_real.shape[0], -1).sum(1).mean()\n",
    "\n",
    "    return grad_penalty\n",
    "\n",
    "\n",
    "def g_nonsaturating_loss(fake_pred):\n",
    "    loss = F.softplus(-fake_pred).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def g_path_regularize(fake_img, latents, mean_path_length, decay=0.01):\n",
    "    noise = torch.randn_like(fake_img) / math.sqrt(\n",
    "        fake_img.shape[2] * fake_img.shape[3]\n",
    "    )\n",
    "    grad, = autograd.grad(\n",
    "        outputs=(fake_img * noise).sum(), inputs=latents, create_graph=True\n",
    "    )\n",
    "    path_lengths = torch.sqrt(grad.pow(2).sum(2).mean(1))\n",
    "\n",
    "    path_mean = mean_path_length + decay * (path_lengths.mean() - mean_path_length)\n",
    "\n",
    "    path_penalty = (path_lengths - path_mean).pow(2).mean()\n",
    "\n",
    "    return path_penalty, path_mean.detach(), path_lengths\n",
    "\n",
    "\n",
    "def make_noise(batch, latent_dim, n_noise, device):\n",
    "    if n_noise == 1:\n",
    "        return torch.randn(batch, latent_dim, device=device)\n",
    "\n",
    "    noises = torch.randn(n_noise, batch, latent_dim, device=device).unbind(0)\n",
    "\n",
    "    return noises\n",
    "\n",
    "\n",
    "def mixing_noise(batch, latent_dim, prob, device):\n",
    "    if prob > 0 and random.random() < prob:\n",
    "        return make_noise(batch, latent_dim, 2, device)\n",
    "\n",
    "    else:\n",
    "        return [make_noise(batch, latent_dim, 1, device)]\n",
    "\n",
    "\n",
    "def set_grad_none(model, targets):\n",
    "    for n, p in model.named_parameters():\n",
    "        if n in targets:\n",
    "            p.grad = None\n",
    "\n",
    "def validate(args, d_img, d_seg, val_loader, device, writer, step):\n",
    "    with torch.no_grad():\n",
    "        d_img_val_scores = []\n",
    "        d_seg_val_scores = []\n",
    "        for i, data in enumerate(val_loader):\n",
    "            img, mask = data['image'].to(device), data['mask'].to(device)\n",
    "            \n",
    "            d_img_val_score = d_img(img)\n",
    "            d_seg_val_score = d_seg(prep_dseg_input(args, img, mask, is_real=True))\n",
    "            d_seg_val_score = torch.tensor([feat[-1].mean() for feat in d_seg_val_score])\n",
    "\n",
    "            d_img_val_scores.append(d_img_val_score.mean().item())\n",
    "            d_seg_val_scores.append(d_seg_val_score.mean().item())\n",
    "        \n",
    "        d_img_val_scores = np.array(d_img_val_scores).mean()\n",
    "        d_seg_val_scores = np.array(d_seg_val_scores).mean()\n",
    "\n",
    "        print(\"d_img val scores: {0:.4f}, d_seg val scores: {1:.4f}\".format(d_img_val_scores, d_seg_val_scores))\n",
    "\n",
    "        writer.add_scalar('scores/d_img_val', d_img_val_scores, global_step=step)\n",
    "        writer.add_scalar('scores/d_seg_val', d_seg_val_scores, global_step=step)\n",
    "\n",
    "def prep_dseg_input(args, img, mask, is_real):\n",
    "    dseg_in = torch.cat([img, mask], dim=1)\n",
    "\n",
    "    return dseg_in\n",
    "\n",
    "def prep_dseg_output(args, pred, use_feat=False):\n",
    "    if use_feat:\n",
    "        return pred\n",
    "    else:\n",
    "        for i in range(len(pred)):\n",
    "            for j in range(len(pred[i])-1):\n",
    "                pred[i][j] = pred[i][j].detach()\n",
    "        return pred\n",
    "\n",
    "def create_heatmap(mask_tensor):\n",
    "    mask_np = mask_tensor.detach().cpu().numpy()\n",
    "    batch_size = mask_tensor.shape[0]\n",
    "    heatmap_tensors = []\n",
    "    for i in range(batch_size):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask_np[i][0]), cv2.COLORMAP_JET)\n",
    "        # convert BGR to RGB\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        heatmap_tensor = torch.tensor(heatmap, dtype=torch.float)\n",
    "        heatmap_tensor = heatmap_tensor.permute(2,0,1)\n",
    "        heatmap_tensors.append(heatmap_tensor)\n",
    "    heatmap_tensors = torch.stack(heatmap_tensors, dim=0)\n",
    "    return heatmap_tensors\n",
    "\n",
    "def train(args, ckpt_dir, img_loader, seg_loader, seg_val_loader, generator, discriminator_img,\n",
    "            discriminator_seg, g_optim, d_img_optim, d_seg_optim, g_ema, device, writer):\n",
    "\n",
    "    get_inception_metrics = prepare_inception_metrics(args.inception, False)\n",
    "    # sample func for calculate FID\n",
    "    sample_fn = functools.partial(sample_gema, g_ema=g_ema, device=device, \n",
    "                        truncation=1.0, mean_latent=None, batch_size=args.batch)\n",
    "\n",
    "    # d_seg gan loss\n",
    "    seg_gan_loss = GANLoss(gan_mode='hinge', tensor=torch.cuda.FloatTensor)\n",
    "\n",
    "    img_loader = sample_data(img_loader)\n",
    "    seg_loader = sample_data(seg_loader)\n",
    "    pbar = range(args.iter)\n",
    "\n",
    "    mean_path_length = 0\n",
    "\n",
    "    d_loss_val = 0\n",
    "    r1_img_loss = torch.tensor(0.0, device=device)\n",
    "    r1_seg_loss = torch.tensor(0.0, device=device)\n",
    "    g_loss_val = 0\n",
    "    path_loss = torch.tensor(0.0, device=device)\n",
    "    path_lengths = torch.tensor(0.0, device=device)\n",
    "    mean_path_length_avg = 0\n",
    "    loss_dict = {}\n",
    "\n",
    "    if args.distributed:\n",
    "        g_module = generator.module\n",
    "        d_img_module = discriminator_img.module\n",
    "        d_seg_module = discriminator_seg.module\n",
    "    else:\n",
    "        g_module = generator\n",
    "        d_img_module = discriminator_img\n",
    "        d_seg_module = discriminator_seg\n",
    "        \n",
    "    accum = 0.5 ** (32 / (10 * 1000))\n",
    "\n",
    "    sample_z = torch.randn(args.n_sample, args.latent, device=device)\n",
    "\n",
    "    for idx in pbar:\n",
    "        i = idx + args.start_iter\n",
    "\n",
    "        if i > args.iter:\n",
    "            print('Done!')\n",
    "\n",
    "            break\n",
    "        \n",
    "        # train image and segmentation discriminator\n",
    "        real_img = next(img_loader)['image']\n",
    "\n",
    "        real_img = real_img.to(device)\n",
    "       \n",
    "        seg_data = next(seg_loader)\n",
    "        seg_img, seg_mask = seg_data['image'], seg_data['mask']\n",
    "        seg_img, seg_mask = seg_img.to(device), seg_mask.to(device)\n",
    "\n",
    "        # =============================== Step1: train the d_img ===================================\n",
    "        requires_grad(generator, False)\n",
    "        requires_grad(discriminator_img, True)\n",
    "        requires_grad(discriminator_seg, False)\n",
    "\n",
    "        noise = mixing_noise(args.batch, args.latent, args.mixing, device)\n",
    "        fake_img, fake_seg = generator(noise)\n",
    "\n",
    "        # detach fake seg\n",
    "        fake_seg = fake_seg.detach()\n",
    "\n",
    "        fake_img_pred = discriminator_img(fake_img)\n",
    "        \n",
    "        real_img_pred = discriminator_img(real_img)\n",
    "\n",
    "        d_img_loss = d_logistic_loss(real_img_pred, fake_img_pred)\n",
    "\n",
    "        loss_dict['d_img'] = d_img_loss\n",
    "        loss_dict['d_img_real_score'] = real_img_pred.mean()\n",
    "        loss_dict['d_img_fake_score'] = fake_img_pred.mean()\n",
    "\n",
    "        discriminator_img.zero_grad()\n",
    "        d_img_loss.backward()\n",
    "        d_img_optim.step()\n",
    "        \n",
    "        # =============================== Step2: train the d_seg ===================================\n",
    "        requires_grad(generator, False)\n",
    "        requires_grad(discriminator_img, False)\n",
    "        requires_grad(discriminator_seg, True)\n",
    "\n",
    "        noise = mixing_noise(args.batch, args.latent, args.mixing, device)\n",
    "        fake_img, fake_seg = generator(noise)\n",
    "\n",
    "        fake_seg_pred = discriminator_seg(prep_dseg_input(args, fake_img, fake_seg, is_real=False))\n",
    "        real_seg_pred = discriminator_seg(prep_dseg_input(args, seg_img, seg_mask, is_real=True))\n",
    "\n",
    "        # prepare output\n",
    "        fake_seg_pred = prep_dseg_output(args, fake_seg_pred, use_feat=False)\n",
    "        real_seg_pred = prep_dseg_output(args, real_seg_pred, use_feat=False)\n",
    "\n",
    "        d_seg_loss = (seg_gan_loss(fake_seg_pred, False, for_discriminator=True).mean() + seg_gan_loss(real_seg_pred, True, for_discriminator=True).mean()) / 2.0\n",
    "     \n",
    "        loss_dict['d_seg'] = d_seg_loss\n",
    "        loss_dict['d_seg_real_score'] = (real_seg_pred[0][-1].mean()+real_seg_pred[1][-1].mean()+real_seg_pred[2][-1].mean()) / 3.0\n",
    "        loss_dict['d_seg_fake_score'] = (fake_seg_pred[0][-1].mean()+fake_seg_pred[1][-1].mean()+fake_seg_pred[2][-1].mean()) / 3.0\n",
    "\n",
    "        discriminator_seg.zero_grad()\n",
    "        d_seg_loss.backward()\n",
    "        d_seg_optim.step()\n",
    "\n",
    "        d_regularize = i % args.d_reg_every == 0\n",
    "\n",
    "        if d_regularize:\n",
    "            real_img.requires_grad = True\n",
    "            real_pred = discriminator_img(real_img)\n",
    "            r1_img_loss = d_r1_loss(real_pred, real_img)\n",
    "\n",
    "            discriminator_img.zero_grad()\n",
    "            (args.r1 / 2 * r1_img_loss * args.d_reg_every + 0 * real_pred[0]).backward()\n",
    "\n",
    "            d_img_optim.step()\n",
    "            \n",
    "            # seg discriminator regulate\n",
    "            real_img_seg = prep_dseg_input(args, seg_img, seg_mask, is_real=True)\n",
    "            real_img_seg.requires_grad = True\n",
    " \n",
    "            real_pred = discriminator_seg(real_img_seg)\n",
    "            real_pred = prep_dseg_output(args, real_pred, use_feat=False)\n",
    "\n",
    "            # select three D\n",
    "            real_pred = real_pred[0][-1].mean() + real_pred[1][-1].mean() + real_pred[2][-1].mean()\n",
    "\n",
    "            r1_seg_loss = d_r1_loss(real_pred, real_img_seg)\n",
    "            \n",
    "            discriminator_seg.zero_grad()\n",
    "            (args.r1 / 2 * r1_seg_loss * args.d_reg_every + 0 * real_pred).backward()\n",
    "\n",
    "            d_seg_optim.step()\n",
    "\n",
    "        loss_dict['r1_img'] = r1_img_loss\n",
    "        loss_dict['r1_seg'] = r1_seg_loss\n",
    "\n",
    "        # =============================== Step3: train the generator ===================================\n",
    "        requires_grad(generator, True)\n",
    "        requires_grad(discriminator_img, False)\n",
    "        requires_grad(discriminator_seg, False)\n",
    "\n",
    "        noise = mixing_noise(args.batch, args.latent, args.mixing, device)\n",
    "        fake_img, fake_seg = generator(noise)\n",
    "\n",
    "        fake_img_pred = discriminator_img(fake_img)\n",
    " \n",
    "        # stop gradient from d_seg to g_img\n",
    "        fake_seg_pred = discriminator_seg(prep_dseg_input(args, fake_img.detach(), fake_seg, is_real=False))\n",
    "        real_seg_pred = discriminator_seg(prep_dseg_input(args, seg_img, seg_mask, is_real=True))\n",
    "\n",
    "        # prepare output\n",
    "        fake_seg_pred = prep_dseg_output(args, fake_seg_pred, use_feat=True)\n",
    "        real_seg_pred = prep_dseg_output(args, real_seg_pred, use_feat=False)\n",
    "\n",
    "        g_img_loss = g_nonsaturating_loss(fake_img_pred)\n",
    "        \n",
    "        # g seg adv loss\n",
    "        g_seg_adv_loss = seg_gan_loss(fake_seg_pred, True, for_discriminator=False).mean()\n",
    "\n",
    "        # g seg feat loss\n",
    "        g_seg_feat_loss = 0.0\n",
    "        feat_weights = 1.0\n",
    "        D_weights = 1.0 / 3.0\n",
    "\n",
    "        for D_i in range(len(fake_seg_pred)):\n",
    "            for D_j in range(len(fake_seg_pred[D_i])-1):\n",
    "                g_seg_feat_loss += D_weights * feat_weights * \\\n",
    "                    F.l1_loss(fake_seg_pred[D_i][D_j], real_seg_pred[D_i][D_j].detach()) * args.lambda_dseg_feat\n",
    "\n",
    "        g_loss = g_img_loss + g_seg_adv_loss + g_seg_feat_loss\n",
    "  \n",
    "        loss_dict['g_img'] = g_img_loss\n",
    "        loss_dict['g_seg_adv'] = g_seg_adv_loss\n",
    "        loss_dict['g_seg_feat'] = g_seg_feat_loss\n",
    "        loss_dict['g'] = g_loss\n",
    "\n",
    "        generator.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "        g_regularize = i % args.g_reg_every == 0\n",
    "\n",
    "        if g_regularize:\n",
    "            path_batch_size = max(1, args.batch // args.path_batch_shrink)\n",
    "            noise = mixing_noise(\n",
    "                path_batch_size, args.latent, args.mixing, device\n",
    "            )\n",
    "            fake_img, latents = generator(noise, return_latents=True)\n",
    "\n",
    "            path_loss, mean_path_length, path_lengths = g_path_regularize(\n",
    "                fake_img, latents, mean_path_length\n",
    "            )\n",
    "\n",
    "            generator.zero_grad()\n",
    "            weighted_path_loss = args.path_regularize * args.g_reg_every * path_loss\n",
    "\n",
    "            if args.path_batch_shrink:\n",
    "                weighted_path_loss += 0 * fake_img[0, 0, 0, 0]\n",
    "\n",
    "            weighted_path_loss.backward()\n",
    "      \n",
    "            g_optim.step()\n",
    "\n",
    "            mean_path_length_avg = (\n",
    "                reduce_sum(mean_path_length).item() / get_world_size()\n",
    "            )\n",
    "\n",
    "        loss_dict['path'] = path_loss\n",
    "        loss_dict['path_length'] = path_lengths.mean()\n",
    "\n",
    "        accumulate(g_ema, g_module, accum)\n",
    "\n",
    "        loss_reduced = reduce_loss_dict(loss_dict)\n",
    "\n",
    "        d_img_loss_val = loss_reduced['d_img'].mean().item()\n",
    "        d_seg_loss_val = loss_reduced['d_seg'].mean().item()\n",
    "        g_loss_val = loss_reduced['g'].mean().item()\n",
    "        g_img_loss_val = loss_reduced['g_img'].mean().item()\n",
    "\n",
    "        g_seg_adv_loss_val = loss_reduced['g_seg_adv'].mean().item()\n",
    "        g_seg_feat_loss_val = loss_reduced['g_seg_feat'].mean().item()\n",
    "        r1_img_val = loss_reduced['r1_img'].mean().item()\n",
    "        r1_seg_val = loss_reduced['r1_seg'].mean().item()\n",
    "        \n",
    "\n",
    "        d_img_real_score_val = loss_reduced['d_img_real_score'].mean().item()\n",
    "        d_img_fake_score_val = loss_reduced['d_img_fake_score'].mean().item()\n",
    "        d_seg_real_score_val = loss_reduced['d_seg_real_score'].mean().item()\n",
    "        d_seg_fake_score_val = loss_reduced['d_seg_fake_score'].mean().item()\n",
    "\n",
    "        path_loss_val = loss_reduced['path'].mean().item()\n",
    "        path_length_val = loss_reduced['path_length'].mean().item()\n",
    "\n",
    "        if get_rank() == 0:\n",
    "            # write to tensorboard\n",
    "            writer.add_scalars('scores/d_img',{'real_score': d_img_real_score_val,\n",
    "                                                'fake_score': d_img_fake_score_val\n",
    "                                                    }, global_step=i)\n",
    "\n",
    "            writer.add_scalars('scores/d_seg',{'real_score': d_seg_real_score_val,\n",
    "                                                'fake_score': d_seg_fake_score_val\n",
    "                                                    }, global_step=i)\n",
    "            \n",
    "            writer.add_scalar('r1/d_img', r1_img_val, global_step=i)\n",
    "            writer.add_scalar('r1/d_seg', r1_seg_val, global_step=i)\n",
    "\n",
    "            writer.add_scalar('path/path_loss', path_loss_val, global_step=i)\n",
    "            writer.add_scalar('path/path_length', path_length_val, global_step=i)\n",
    "\n",
    "            writer.add_scalar('g/img_loss', g_img_loss_val, global_step=i)\n",
    "            writer.add_scalar('g/seg_adv_loss', g_seg_adv_loss_val, global_step=i)\n",
    "            writer.add_scalar('g/seg_feat_loss', g_seg_feat_loss_val, global_step=i)\n",
    "\n",
    "\n",
    "            if i % args.viz_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    g_ema.eval()\n",
    "                    sample_img, sample_seg = g_ema([sample_z])\n",
    "                    sample_img = sample_img.detach().cpu()\n",
    "                    sample_seg = sample_seg.detach().cpu()\n",
    "\n",
    "                    if args.seg_name == 'celeba-mask':\n",
    "                        sample_seg = torch.argmax(sample_seg, dim=1)\n",
    "                        color_map = seg_val_loader.dataset.color_map\n",
    "                        sample_mask = torch.zeros((sample_seg.shape[0], sample_seg.shape[1], sample_seg.shape[2], 3), dtype=torch.float)\n",
    "                        for key in color_map:\n",
    "                            sample_mask[sample_seg==key] = torch.tensor(color_map[key], dtype=torch.float)\n",
    "                        sample_mask = sample_mask.permute(0,3,1,2)\n",
    "                    \n",
    "                    else:\n",
    "                        raise Exception('No such a dataloader!')\n",
    "\n",
    "                    os.makedirs(os.path.join(ckpt_dir, 'sample'), exist_ok=True)\n",
    "                    utils.save_image(\n",
    "                        sample_img,\n",
    "                        os.path.join(ckpt_dir, f'sample/img_{str(i).zfill(6)}.png'),\n",
    "                        nrow=int(args.n_sample ** 0.5),\n",
    "                        normalize=True,\n",
    "                        range=(-1, 1),\n",
    "                    )\n",
    "                    \n",
    "                    utils.save_image(\n",
    "                            sample_mask,\n",
    "                            os.path.join(ckpt_dir, f'sample/mask_{str(i).zfill(6)}.png'),\n",
    "                            nrow=int(args.n_sample ** 0.5),\n",
    "                            normalize=True,\n",
    "                    )\n",
    "\n",
    "            \n",
    "\n",
    "            if i % args.eval_every == 0:\n",
    "                print(\"==================Start calculating validation scores==================\")\n",
    "                validate(args, discriminator_img, discriminator_seg, seg_val_loader, device, writer, i)\n",
    "                \n",
    "            if i % args.save_every == 0:\n",
    "                print(\"==================Start calculating FID==================\")\n",
    "                IS_mean, IS_std, FID = get_inception_metrics(sample_fn, num_inception_images=10000, use_torch=False)\n",
    "                print(\"iteration {0:08d}: FID: {1:.4f}, IS_mean: {2:.4f}, IS_std: {3:.4f}\".format(i, FID, IS_mean, IS_std))\n",
    "\n",
    "\n",
    "                writer.add_scalar('metrics/FID', FID, global_step=i)\n",
    "                writer.add_scalar('metrics/IS_mean', IS_mean, global_step=i)\n",
    "                writer.add_scalar('metrics/IS_std', IS_std, global_step=i)\n",
    "\n",
    "                writer.add_text('metrics/FID', 'FID is {0:.4f}'.format(FID), global_step=i)\n",
    "                \n",
    "                \n",
    "                os.makedirs(os.path.join(ckpt_dir, 'ckpt'), exist_ok=True)\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'g': g_module.state_dict(),\n",
    "                        'd_img': d_img_module.state_dict(),\n",
    "                        'd_seg': d_seg_module.state_dict(),\n",
    "                        'g_ema': g_ema.state_dict(),\n",
    "                        'args': args,\n",
    "                    },\n",
    "                    os.path.join(ckpt_dir, f'ckpt/{str(i).zfill(6)}.pt'),\n",
    "                )\n",
    "\n",
    "def get_seg_dataset(args, phase='train'):\n",
    "    if args.seg_name == 'celeba-mask':\n",
    "        seg_dataset = CelebAMaskDataset(args, args.seg_dataset, is_label=True, phase=phase,\n",
    "                                            limit_size=args.limit_data, aug=args.seg_aug, resolution=args.size)\n",
    "   \n",
    "    else:\n",
    "        raise Exception('No such a dataloader!')\n",
    "    \n",
    "    return seg_dataset\n",
    "\n",
    "def get_transformation(args):\n",
    "    if args.seg_name == 'celeba-mask':\n",
    "        transform = transforms.Compose(\n",
    "                    [\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5), inplace=True)\n",
    "                    ]\n",
    "                )\n",
    "    \n",
    "    else:\n",
    "        raise Exception('No such a dataloader!')\n",
    "    \n",
    "    return transform\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = 'cuda'\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--img_dataset', type=str, required=True)\n",
    "    parser.add_argument('--seg_dataset', type=str, required=True)\n",
    "    parser.add_argument('--inception', type=str, help='inception pkl', required=True)\n",
    "\n",
    "    parser.add_argument('--seg_name', type=str, help='segmentation dataloader name[celeba-mask]', default='celeba-mask')\n",
    "    parser.add_argument('--iter', type=int, default=800000)\n",
    "    parser.add_argument('--batch', type=int, default=16)\n",
    "    parser.add_argument('--n_sample', type=int, default=64)\n",
    "    parser.add_argument('--size', type=int, default=256)\n",
    "    parser.add_argument('--r1', type=float, default=10)\n",
    "    parser.add_argument('--path_regularize', type=float, default=2)\n",
    "    parser.add_argument('--path_batch_shrink', type=int, default=2)\n",
    "    parser.add_argument('--d_reg_every', type=int, default=16)\n",
    "    parser.add_argument('--g_reg_every', type=int, default=4)\n",
    "    parser.add_argument('--d_use_seg_every', type=int, help='frequency mixing seg image with real image', default=-1)\n",
    "    parser.add_argument('--viz_every', type=int, default=100)\n",
    "    parser.add_argument('--eval_every', type=int, default=1000)\n",
    "    parser.add_argument('--save_every', type=int, default=2000)\n",
    "\n",
    "    parser.add_argument('--mixing', type=float, default=0.9)\n",
    "    parser.add_argument('--lambda_dseg_feat', type=float, default=2.0)\n",
    "    parser.add_argument('--ckpt', type=str, default=None)\n",
    "    parser.add_argument('--lr', type=float, default=0.002)\n",
    "    parser.add_argument('--channel_multiplier', type=int, default=2)\n",
    "    \n",
    "    parser.add_argument('--limit_data', type=str, default=None, help='number of limited label data point to use')\n",
    "    parser.add_argument('--unlabel_limit_data', type=str, default=None, help='number of limited unlabel data point to use')\n",
    "\n",
    "    parser.add_argument('--image_mode', type=str, default='RGB', help='Image mode RGB|L')\n",
    "    parser.add_argument('--seg_dim', type=int, default=8)\n",
    "    parser.add_argument('--seg_aug', action='store_true', help='seg augmentation')\n",
    "\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoint/')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # build checkpoint dir\n",
    "    from datetime import datetime\n",
    "    current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    ckpt_dir = os.path.join(args.checkpoint_dir, 'run-'+current_time)\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    #writer = SummaryWriter(log_dir=os.path.join(ckpt_dir, 'logs'))\n",
    "\n",
    "    n_gpu = int(os.environ['WORLD_SIZE']) if 'WORLD_SIZE' in os.environ else 1\n",
    "    args.n_gpu = n_gpu\n",
    "\n",
    "    args.distributed = n_gpu > 1\n",
    "\n",
    "    if args.distributed:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "        synchronize()\n",
    "\n",
    "    args.latent = 512\n",
    "    args.n_mlp = 8\n",
    "\n",
    "    args.start_iter = 0\n",
    "\n",
    "    generator = GeneratorSeg(\n",
    "        args.size, args.latent, args.n_mlp, seg_dim=args.seg_dim,\n",
    "        image_mode=args.image_mode, channel_multiplier=args.channel_multiplier\n",
    "    ).to(device)\n",
    "\n",
    "    if args.image_mode == 'RGB':\n",
    "        d_input_dim = 3\n",
    "    else:\n",
    "        d_input_dim = 1\n",
    "\n",
    "    d_seg_input_dim = d_input_dim + args.seg_dim\n",
    "\n",
    "    discriminator_img = Discriminator(\n",
    "        args.size, input_dim=d_input_dim, channel_multiplier=args.channel_multiplier\n",
    "    ).to(device)\n",
    "\n",
    "    discriminator_seg = MultiscaleDiscriminator(input_nc=d_seg_input_dim, getIntermFeat=True).to(device)\n",
    " \n",
    "    g_ema = GeneratorSeg(\n",
    "        args.size, args.latent, args.n_mlp, seg_dim=args.seg_dim,\n",
    "        image_mode=args.image_mode, channel_multiplier=args.channel_multiplier\n",
    "    ).to(device)\n",
    "    g_ema.eval()\n",
    "    accumulate(g_ema, generator, 0)\n",
    "\n",
    "    g_reg_ratio = args.g_reg_every / (args.g_reg_every + 1)\n",
    "    d_reg_ratio = args.d_reg_every / (args.d_reg_every + 1)\n",
    "\n",
    "    g_optim = optim.Adam(\n",
    "        generator.parameters(),\n",
    "        lr=args.lr * g_reg_ratio,\n",
    "        betas=(0 ** g_reg_ratio, 0.99 ** g_reg_ratio),\n",
    "    )\n",
    "    d_img_optim = optim.Adam(\n",
    "        discriminator_img.parameters(),\n",
    "        lr=args.lr * d_reg_ratio,\n",
    "        betas=(0 ** d_reg_ratio, 0.99 ** d_reg_ratio),\n",
    "    )\n",
    "    d_seg_optim = optim.Adam(\n",
    "        discriminator_seg.parameters(),\n",
    "        lr=args.lr * d_reg_ratio,\n",
    "        betas=(0 ** d_reg_ratio, 0.99 ** d_reg_ratio),\n",
    "    )\n",
    "\n",
    "    if args.ckpt is not None:\n",
    "        print('load model:', args.ckpt)\n",
    "        \n",
    "        ckpt = torch.load(args.ckpt)\n",
    "\n",
    "        try:\n",
    "            ckpt_name = os.path.basename(args.ckpt)\n",
    "            args.start_iter = int(os.path.splitext(ckpt_name)[0])\n",
    "            \n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "        generator.load_state_dict(ckpt['g'])\n",
    "        discriminator_img.load_state_dict(ckpt['d_img'])\n",
    "        discriminator_seg.load_state_dict(ckpt['d_seg'])\n",
    "        g_ema.load_state_dict(ckpt['g_ema'])\n",
    "\n",
    "        g_optim.load_state_dict(ckpt['g_optim'])\n",
    "        d_img_optim.load_state_dict(ckpt['d_img_optim'])\n",
    "        d_seg_optim.load_state_dict(ckpt['d_seg_optim'])\n",
    "\n",
    "    if args.distributed:\n",
    "        generator = nn.parallel.DistributedDataParallel(\n",
    "            generator,\n",
    "            device_ids=[args.local_rank],\n",
    "            output_device=args.local_rank,\n",
    "            broadcast_buffers=False,\n",
    "            find_unused_parameters=True,\n",
    "        )\n",
    "\n",
    "        discriminator_img = nn.parallel.DistributedDataParallel(\n",
    "            discriminator_img,\n",
    "            device_ids=[args.local_rank],\n",
    "            output_device=args.local_rank,\n",
    "            broadcast_buffers=False,\n",
    "            find_unused_parameters=True,\n",
    "        )\n",
    "\n",
    "        discriminator_seg = nn.parallel.DistributedDataParallel(\n",
    "            discriminator_seg,\n",
    "            device_ids=[args.local_rank],\n",
    "            output_device=args.local_rank,\n",
    "            broadcast_buffers=False,\n",
    "            find_unused_parameters=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    if args.seg_name == 'celeba-mask':\n",
    "        transform = get_transformation(args)\n",
    "        img_dataset = CelebAMaskDataset(args, args.img_dataset, unlabel_transform=transform, unlabel_limit_size=args.unlabel_limit_data,\n",
    "                                                is_label=False, resolution=args.size)\n",
    "    else:\n",
    "        raise Exception('No such a dataloader!')\n",
    "\n",
    "    print(\"Loading unlabel dataloader with size \", img_dataset.data_size)\n",
    "\n",
    "    img_loader = data.DataLoader(\n",
    "        img_dataset,\n",
    "        batch_size=args.batch,\n",
    "        sampler=data_sampler(img_dataset, shuffle=True, distributed=args.distributed),\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    seg_dataset = get_seg_dataset(args, phase='train')\n",
    "\n",
    "    print(\"Loading train dataloader with size \", seg_dataset.data_size)\n",
    "\n",
    "    seg_loader = data.DataLoader(\n",
    "        seg_dataset,\n",
    "        batch_size=args.batch,\n",
    "        sampler=data_sampler(seg_dataset, shuffle=True, distributed=args.distributed),\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    seg_val_dataset = get_seg_dataset(args, phase='val')\n",
    "\n",
    "    print(\"Loading val dataloader with size \", seg_val_dataset.data_size)\n",
    "\n",
    "    seg_val_loader = data.DataLoader(\n",
    "        seg_val_dataset,\n",
    "        batch_size=args.batch,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    train(args, ckpt_dir, img_loader, seg_loader, seg_val_loader, generator, discriminator_img, discriminator_seg,\n",
    "                    g_optim, d_img_optim, d_seg_optim, g_ema, device, writer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
